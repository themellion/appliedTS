---
title: "Linear Regression Model"
---

<style>
body {
text-align: justify}
</style>

```
```


### Multiple linear regression model
In this section we will introduce multiplie linear regression and its use in forecasting problems. Lets assume that company ABC produces a detergent. The company would like to develop a prediction model that can be used to predict the demand for the detergent. The demand for the detergent (in hundreds of thousands of bottles) in "sales period t", where each sales period is defined to last 4 weeks, is denoted by $y_t$ and is called the *dependent variable*. The demand for ABC's detergent is partially determined by one or more of the *independent variables*, $x_{t1}$ = the price of the detergent in period *t*, $x_{t2}$ = the average industry price in period *t* of competitors' similar detergents, and $x_{t3}$ = the advertising expenditure in period *t* of ABC to promote the detergent. The data below have been observed in the last 30 sales periods. 

```{r echo = FALSE, results = 'asis'}
library('knitr')
library('kableExtra')
# library('hwriter')
sales <- data.frame('t' = 1:30, 'Demand' = c(7.38, 8.51, 9.52, 7.50, 9.33, 8.28, 8.75, 7.87, 7.10, 8.00, 7.89, 8.15, 9.10, 8.86, 8.90, 8.87, 9.26, 9.00, 8.75, 7.95, 7.65, 7.27, 8.00, 8.50, 8.75, 9.21, 8.27, 7.67, 7.93, 9.26), 'Price' = c(3.85, 3.75, 3.70, 3.70, 3.60, 3.60, 3.60, 3.80, 3.80, 3.85, 3.90, 3.90, 3.70, 3.75, 3.75, 3.80, 3.70, 3.80, 3.70, 3.80, 3.80, 3.75, 3.70, 3.55, 3.60, 3.65, 3.70, 3.75, 3.80, 3.70), 'Average Industry Price' = c(3.80, 4.00, 4.30, 3.70, 3.85, 3.80, 3.75, 3.85, 3.65, 4.00, 4.10, 4.00, 4.10, 4.20, 4.10, 4.10, 4.20, 4.30, 4.10, 3.75, 3.75, 3.65, 3.90, 3.65, 4.10, 4.25, 3.65, 3.75, 3.85, 4.25), 'Advertising Expenditure' = c(5.50, 6.75, 7.25, 5.50, 7.00, 6.50, 6.75, 5.25, 5.25, 6.00, 6.50, 6.25, 7.00, 6.90, 6.80, 6.80, 7.10, 7.00, 6.80, 6.50, 6.25, 6.00, 6.50, 7.00, 6.80, 6.80, 6.50, 5.75, 5.80, 6.80), check.names = FALSE)

# cat(hwrite(sales, center=TRUE, width='300px', table.style='padding: 50px', row.names=FALSE, row.style=list('font-weight:bold')))

knitr::kable(sales, caption = 'Historical data relevant to the demand for ABC detergent') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```


Assuming ABC can fairly easily determine $x_{t1}$, $x_{t2}$ and $x_{t3}$ for a future period *t*, using the above data we can develop a relationship that expresses $y_t$ as a function of $x_{t1}$, $x_{t2}$ and $x_{t3}$ and forecast $y_t$. One possible relationship is the following multiple regression model:$$ y_t = \beta_0 + \beta_1 x_{t1} + \beta_2 x_{t2} + \beta_3 x_{t3} + \epsilon_t$$ where $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$ are unknown constants or numbers. These constants are often called parameters and relate the dependent variable $y_t$ to the independent variables $x_{t1}$, $x_{t2}$ and $x_{t3}$. The term $\epsilon_t$ is an irregular component often called a random error component. **The random error component describes the combined influence on $y_t$ of factors others than the independent variables $x_{t1}$, $x_{t2}$ and $x_{t3}$**. Two such factors are measurement error, which takes into account inaccurate measuring and/or reporting of the values $y_t$ and stochastic error, which takes into account the effect on the dependent variable $y_t$ of all independent variables other than the independent variables $x_{t1}$, $x_{t2}$ and $x_{t3}$ explicitly included in the model. **It is assumed that the expected value of $\epsilon_t$ is 0, which says that in the long run the random errors average out to 0**. For a particular period *t*, however, $\epsilon_t$ will probably not be 0. As a result the model $$ y_t = \beta_0 + \beta_1 x_{t1} + \beta_2 x_{t2} + \beta_3 x_{t3} + \epsilon_t $$ states that the time-series $y_t$ can be represented by an average level which changes over time according to the expression $\beta_0 + \beta_1 x_{t1} + \beta_2 x_{t2} + \beta_3 x_{t3}$ combined with random fluctuations represented by $\epsilon_t$ which cause the observations of the time-series to deviate from the average level. 


Using the data from the table above we can obtain the 'least-squares' estimates:
```{r echo = FALSE}
demand <- lm(Demand ~ Price + `Average Industry Price` + `Advertising Expenditure`, data = sales)
demand$coefficients
```


that is, $b_0$ = 7.5891025, $b_1$ = -2.3577228, $b_2$ = 1.6122144, and $b_3$ = 0.5011517. These estimates are 'guesses' made on the basis of existing data, for values of the unknown parameters $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ which relate the dependent variable $y_t$ to the independent variables $x_{t1}$, $x_{t2}$ and $x_{t3}$. We call $b_0$, $b_1$, $b_2$, and $b_3$ the least squares estimates of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ because these estimates give a value of $\sum_{t=1}^{n}{(y_t - \hat{y}_t)^2}$ that is smaller than any other estimates would give. 

Usually we want to know 'how far' $\hat{y}_t$ differ from $y_t$ and the average level, $\mu_t$, when $\epsilon_t$ = 0. In order to obtain this information we need to find the confidence interval for $y_t$ and $\mu_t$. Finding the confidence intervals requires the following conditions to hold:

* for each and every period *t*, the error component $\epsilon_t$ follows a normal probability distribution. 
* the variance of $y_t$ is the same for each and every value of *t*. 
* the time-series values $y_1, y_2, \ldots$ in different periods are statistically independent of or not related to each other. 

```
```

